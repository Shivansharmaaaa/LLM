{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "with open('wiz_of_oz.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "vocab_size=len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60, 61, 1, 72, 60, 57, 70, 57]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode(\"hi there\"))\n",
    "print(decode(encode(\"hi there\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([226639]) torch.int64\n",
      "tensor([ 0,  0, 26, 31, 24, 39, 43, 28, 41,  1, 12, 10,  0,  0, 43, 31, 28,  1,\n",
      "        28, 24, 41, 43, 31, 40, 44, 24, 34, 28,  0,  0,  0, 43, 60, 57,  1, 72,\n",
      "        70, 53, 61, 66,  1, 58, 70, 67, 65,  1,  5, 29, 70, 61, 71, 55, 67,  1,\n",
      "        75, 53, 71,  1, 74, 57, 70, 77,  1, 64, 53, 72, 57, 10,  1, 32, 72,  1,\n",
      "        71, 60, 67, 73, 64, 56,  1, 60, 53, 74, 57,  1, 53, 70, 70, 61, 74, 57,\n",
      "        56,  1, 53, 72,  1, 31, 73, 59, 71, 67, 66,  5, 71,  0, 71, 61, 56, 61,\n",
      "        66, 59,  1, 53, 72,  1, 65, 61, 56, 66, 61, 59, 60, 72,  8,  1, 54, 73,\n",
      "        72,  1, 61, 72,  1, 75, 53, 71,  1, 53, 64, 70, 57, 53, 56, 77,  1, 58,\n",
      "        61, 74, 57,  1, 67,  5, 55, 64, 67, 55, 63,  1, 53, 66, 56,  1, 72, 60,\n",
      "        57,  1, 59, 70, 53, 77,  1, 56, 53, 75, 66,  0, 75, 53, 71,  1, 54, 70,\n",
      "        57, 53, 63, 61, 66, 59,  1, 61, 66,  1, 72, 60, 57,  1, 57, 53, 71, 72,\n",
      "         1, 75, 60, 57, 66,  1, 72, 60, 57,  1, 64, 61, 72, 72, 64, 57,  1, 72,\n",
      "        70, 53, 61, 66,  1, 71, 64, 67, 75, 64, 77,  1, 70, 73, 65, 54, 64, 57,\n",
      "        56,  1, 73, 68,  1, 72, 67,  1, 72, 60, 57,  0, 67, 68, 57, 66,  1, 71,\n",
      "        60, 57, 56,  1, 72, 60, 53, 72,  1, 71, 57, 70, 74, 57, 56,  1, 58, 67,\n",
      "        70,  1, 72, 60, 57,  1, 71, 72, 53, 72, 61, 67, 66,  9, 60, 67, 73, 71,\n",
      "        57, 10,  1, 24, 71,  1, 61, 72,  1, 55, 53, 65, 57,  1, 72, 67,  1, 53,\n",
      "         1, 71, 72, 67, 68,  1, 72, 60, 57,  0, 55, 67, 66, 56, 73, 55, 72, 67,\n",
      "        70,  1, 55, 53, 64, 64, 57, 56,  1, 67, 73, 72,  1, 61, 66,  1, 53,  1,\n",
      "        64, 67, 73, 56,  1, 74, 67, 61, 55, 57, 21,  0,  0,  3, 31, 73, 59, 71,\n",
      "        67, 66,  5, 71,  1, 42, 61, 56, 61, 66, 59,  2,  3,  0,  0, 24, 72,  1,\n",
      "        67, 66, 55, 57,  1, 53,  1, 64, 61, 72, 72, 64, 57,  1, 59, 61, 70, 64,\n",
      "         1, 70, 67, 71, 57,  1, 58, 70, 67, 65,  1, 60, 57, 70,  1, 71, 57, 53,\n",
      "        72,  1, 53, 66, 56,  1, 75, 53, 64, 63, 57, 56,  1, 72, 67,  1, 72, 60,\n",
      "        57,  1, 56, 67, 67, 70,  1, 67, 58,  1, 72, 60, 57,  0, 55, 53, 70,  8,\n",
      "         1, 55, 53, 70, 70, 77, 61, 66, 59,  1, 53,  1, 75, 61, 55, 63, 57, 70,\n",
      "         1, 71, 73, 61, 72,  9, 55, 53, 71, 57,  1, 61, 66,  1, 67, 66, 57,  1,\n",
      "        60, 53, 66, 56,  1, 53, 66, 56,  1, 53,  1, 70, 67, 73, 66, 56,  1, 54,\n",
      "        61, 70, 56,  9, 55, 53, 59, 57,  0, 55, 67, 74, 57, 70, 57, 56,  1, 73,\n",
      "        68,  1, 75, 61, 72, 60,  1, 66, 57, 75, 71, 68, 53, 68, 57, 70, 71,  1,\n",
      "        61, 66,  1, 72, 60, 57,  1, 67, 72, 60, 57, 70,  8,  1, 75, 60, 61, 64,\n",
      "        57,  1, 53,  1, 68, 53, 70, 53, 71, 67, 64,  1, 75, 53, 71,  1, 72, 73,\n",
      "        55, 63, 57, 56,  0, 73, 66, 56, 57, 70,  1, 60, 57, 70,  1, 53, 70, 65,\n",
      "        10,  1, 43, 60, 57,  1, 55, 67, 66, 56, 73, 55, 72, 67, 70,  1, 60, 57,\n",
      "        64, 68, 57, 56,  1, 60, 57, 70,  1, 67, 58, 58,  1, 72, 60, 57,  1, 55,\n",
      "        53, 70,  1, 53, 66, 56,  1, 72, 60, 57, 66,  1, 72, 60, 57,  0, 57, 66,\n",
      "        59, 61, 66, 57, 57, 70,  1, 71, 72, 53, 70, 72, 57, 56,  1, 60, 61, 71,\n",
      "         1, 72, 70, 53, 61, 66,  1, 53, 59, 53, 61, 66,  8,  1, 71, 67,  1, 72,\n",
      "        60, 53, 72,  1, 61, 72,  1, 68, 73, 58, 58, 57, 56,  1, 53, 66, 56,  1,\n",
      "        59, 70, 67, 53, 66, 57, 56,  1, 53, 66, 56,  0, 65, 67, 74, 57, 56,  1,\n",
      "        71, 64, 67, 75, 64, 77,  1, 53, 75, 53, 77,  1, 73, 68,  1, 72, 60, 57,\n",
      "         1, 72, 70, 53, 55, 63, 10,  1, 43, 60, 57,  1, 70, 57, 53, 71, 67, 66,\n",
      "         1, 60, 57,  1, 75, 53, 71,  1, 71, 67,  1, 64, 53, 72, 57,  1, 75, 53,\n",
      "        71,  1, 54, 57, 55, 53, 73, 71, 57,  0, 53, 64, 64,  1, 72, 60, 70, 67,\n",
      "        73, 59, 60,  1, 72, 60, 57,  1, 66, 61, 59, 60, 72,  1, 72, 60, 57, 70,\n",
      "        57,  1, 75, 57, 70, 57,  1, 72, 61, 65, 57, 71,  1, 75, 60, 57, 66,  1,\n",
      "        72, 60, 57,  1, 71, 67, 64, 61, 56,  1, 57, 53, 70, 72, 60,  1, 71, 60,\n",
      "        67, 67, 63,  1, 53, 66, 56,  0, 72, 70, 57, 65, 54, 64, 57, 56,  1, 73,\n",
      "        66, 56, 57, 70,  1, 60, 61, 65,  8,  1, 53, 66, 56,  1, 72, 60, 57,  1,\n",
      "        57, 66, 59, 61, 66, 57, 57, 70,  1, 75, 53, 71,  1, 53, 58, 70, 53, 61,\n",
      "        56,  1, 72, 60, 53, 72,  1, 53, 72,  1, 53, 66, 77,  1, 65, 67, 65, 57,\n",
      "        66, 72,  1, 72, 60, 57,  0, 70, 53, 61, 64, 71,  1, 65, 61, 59, 60, 72,\n",
      "         1, 71, 68, 70, 57, 53, 56,  1, 53, 68, 53, 70, 72,  1, 53, 66, 56,  1,\n",
      "        53, 66,  1, 53, 55, 55, 61, 56, 57, 66, 72,  1, 60, 53, 68, 68, 57, 66,\n",
      "         1, 72, 67,  1, 60, 61, 71,  1, 68, 53, 71, 71, 57, 66, 59, 57, 70, 71,\n",
      "        10,  1, 42, 67,  1, 60, 57,  0, 65, 67])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=int(0.9*len(data))\n",
    "train_data=data[:n]\n",
    "val_data=data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0, 26, 31, 24, 39, 43, 28, 41])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size=8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when context is tensor([0]) the target is 0:\n",
      "when context is tensor([0, 0]) the target is 26:\n",
      "when context is tensor([ 0,  0, 26]) the target is 31:\n",
      "when context is tensor([ 0,  0, 26, 31]) the target is 24:\n",
      "when context is tensor([ 0,  0, 26, 31, 24]) the target is 39:\n",
      "when context is tensor([ 0,  0, 26, 31, 24, 39]) the target is 43:\n",
      "when context is tensor([ 0,  0, 26, 31, 24, 39, 43]) the target is 28:\n",
      "when context is tensor([ 0,  0, 26, 31, 24, 39, 43, 28]) the target is 41:\n"
     ]
    }
   ],
   "source": [
    "x= train_data[:block_size]\n",
    "y= train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context=x[:t+1]\n",
    "    target=y[t]\n",
    "    print(f\"when context is {context} the target is {target}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[57, 66,  8,  1, 72, 60, 67, 73],\n",
      "        [ 1, 71, 75, 57, 57, 72,  1, 65],\n",
      "        [57,  8,  1, 54, 57, 55, 53, 73],\n",
      "        [71,  8,  1, 53, 66, 56,  1, 33]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[66,  8,  1, 72, 60, 67, 73, 59],\n",
      "        [71, 75, 57, 57, 72,  1, 65, 73],\n",
      "        [ 8,  1, 54, 57, 55, 53, 73, 71],\n",
      "        [ 8,  1, 53, 66, 56,  1, 33, 61]])\n",
      "------\n",
      "when context is tensor([57]) the target : 66:\n",
      "when context is tensor([57, 66]) the target : 8:\n",
      "when context is tensor([57, 66,  8]) the target : 1:\n",
      "when context is tensor([57, 66,  8,  1]) the target : 72:\n",
      "when context is tensor([57, 66,  8,  1, 72]) the target : 60:\n",
      "when context is tensor([57, 66,  8,  1, 72, 60]) the target : 67:\n",
      "when context is tensor([57, 66,  8,  1, 72, 60, 67]) the target : 73:\n",
      "when context is tensor([57, 66,  8,  1, 72, 60, 67, 73]) the target : 59:\n",
      "when context is tensor([1]) the target : 71:\n",
      "when context is tensor([ 1, 71]) the target : 75:\n",
      "when context is tensor([ 1, 71, 75]) the target : 57:\n",
      "when context is tensor([ 1, 71, 75, 57]) the target : 57:\n",
      "when context is tensor([ 1, 71, 75, 57, 57]) the target : 72:\n",
      "when context is tensor([ 1, 71, 75, 57, 57, 72]) the target : 1:\n",
      "when context is tensor([ 1, 71, 75, 57, 57, 72,  1]) the target : 65:\n",
      "when context is tensor([ 1, 71, 75, 57, 57, 72,  1, 65]) the target : 73:\n",
      "when context is tensor([57]) the target : 8:\n",
      "when context is tensor([57,  8]) the target : 1:\n",
      "when context is tensor([57,  8,  1]) the target : 54:\n",
      "when context is tensor([57,  8,  1, 54]) the target : 57:\n",
      "when context is tensor([57,  8,  1, 54, 57]) the target : 55:\n",
      "when context is tensor([57,  8,  1, 54, 57, 55]) the target : 53:\n",
      "when context is tensor([57,  8,  1, 54, 57, 55, 53]) the target : 73:\n",
      "when context is tensor([57,  8,  1, 54, 57, 55, 53, 73]) the target : 71:\n",
      "when context is tensor([71]) the target : 8:\n",
      "when context is tensor([71,  8]) the target : 1:\n",
      "when context is tensor([71,  8,  1]) the target : 53:\n",
      "when context is tensor([71,  8,  1, 53]) the target : 66:\n",
      "when context is tensor([71,  8,  1, 53, 66]) the target : 56:\n",
      "when context is tensor([71,  8,  1, 53, 66, 56]) the target : 1:\n",
      "when context is tensor([71,  8,  1, 53, 66, 56,  1]) the target : 33:\n",
      "when context is tensor([71,  8,  1, 53, 66, 56,  1, 33]) the target : 61:\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size=4\n",
    "block_size=8\n",
    "\n",
    "def get_batch(split):\n",
    "    data=train_data if split =='train' else val_data\n",
    "    ix= torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x=torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y=torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y\n",
    "\n",
    "xb,yb=get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('------')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context=xb[b,:t+1]\n",
    "        target=yb[b,t]\n",
    "        print(f\"when context is {context} the target : {target}:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 79])\n",
      "tensor(4.7223, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "D.'CHWmYgV6CiL4Q8k]MKC,?&pSZ2RvV3_rgj-3wI:\n",
      "HE-sTbK?'Sd7xk-T1c5n:u,._ykK?2PQYVIs]o,k nZ!U5fY0vDH4bhcK\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(m.parameters(),lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.64561128616333\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(5000): \n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "YDR\n",
      "blvYM,4OVCY85Amv\n",
      "lW0ILqC2,.)6\"83zMz6leF6A bHhdX(Nu,J0pR_Eg -to,JN2:?epKNLoKLfN\n",
      "iv_EqCHLo'C,z&&Rd:evV\n",
      "i -ANz7EdXFe[SdswQLqkp(:9qw?7aHHxMmvyGyUY'Cm)nuEOzMh1.D\n",
      "8Yh,zZlHJ2Msk2x9DHU_aLWEkYLP;'Jdhy_U8r06k5z7(fBv(dmyigX\n",
      "GdaSzZ wqJs\"8FS]Fa\"8&R2n'EScEu55Al CmQ[R.D!R&wAl1C-Jn\n",
      "Y[f-K\n",
      "hdAFJqJ2x1918aXZ1-\"xzC6\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next 10 numbers in the sequence: [50.65351319789421, 50.61359242346338, 50.57367164903255, 50.533750874601715, 50.49383010017089, 50.45390932574006, 50.413988551309224, 50.37406777687839, 50.33414700244756, 50.294226228016726]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "with open('wiz_of_oz.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "vocab_size=len(chars)\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "X = data[:-1].unsqueeze(1)\n",
    "y = data[1:]\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "next_numbers = []\n",
    "sequence = X[-1][0]\n",
    "for _ in range(10):\n",
    "    next_number = model.predict([[sequence]])\n",
    "    next_numbers.append(next_number[0])\n",
    "    sequence += 1\n",
    "\n",
    "print(\"Next 10 numbers in the sequence:\", next_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence: 16 the an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an an\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def generate_next_tokens(model, start_sequence, max_new_tokens):\n",
    "    start_sequence_encoded = torch.tensor(encode(start_sequence), dtype=torch.long).unsqueeze(0)\n",
    "    new_sequence = start_sequence_encoded.clone()\n",
    "    sequence = start_sequence_encoded\n",
    "    hidden = None\n",
    "    for _ in range(max_new_tokens):\n",
    "        output, hidden = model(sequence, hidden)\n",
    "        sequence = torch.argmax(output, dim=2)\n",
    "        new_sequence = torch.cat((new_sequence, sequence), dim=1)\n",
    "    new_tokens = [itos[i.item()] for i in new_sequence.squeeze()]\n",
    "    return ''.join(new_tokens)\n",
    "\n",
    "with open('wiz_of_oz.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "vocab_size=len(chars)\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, hidden\n",
    "\n",
    "X = data[:-1][:, np.newaxis]\n",
    "y = data[1:]\n",
    "\n",
    "X_tensor = X.clone().detach().long()\n",
    "y_tensor = y.clone().detach().long()\n",
    "\n",
    "model = SimpleRNN(vocab_size, 128, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs, _ = model(X_tensor, None)\n",
    "    loss = criterion(outputs.view(-1, vocab_size), y_tensor.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "generated_sequence = generate_next_tokens(model, '1', 200)\n",
    "print(\"Generated Sequence:\", generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence: 1W\"Tand, treroucand the tofer anjdelneve:rs cighatnon the sebres them cor sle sowed theattirvthe pory,\"\n",
      "slo, eir theint sa'd garore se ar7at them ancerenbwned buonleaagheg Wacher slicg hay wec'9d creedey f mane g si, Wqug sere dir ane wourd my ancer ald he ssthin gu.\n",
      "In,urtt dal antr ullat, wire he ilsth, tac, seme uly toou oroobedun th min band sware he worout linz-ly.\n",
      "\n",
      "folos some\n",
      "det the i'K sineng\" an wemn unmor ond yever ane ove reder now heand ulathe the, sahy wis the?n, Aescou bilf acleE, ard oftard aporoe shat sowit, easlist in The the pla Jashy hhemtintcar Wi giz?bto.\n",
      "\"Wound are.\n",
      "\n",
      "\"O\n",
      "Ithim sare to tped the the thine a[d wou the tterpee\n",
      "\n",
      "igr 6jos hitgneam of,frit pead tithesing re. II sichat warkg, I ther\"\"\n",
      "\n",
      "\"[ur-verarnwir ofdkp jint ilag suate rarp\"(latle rewkikc pilonTt tisy gardoll\n",
      "\"n roth \"\n",
      "oog tht eacedond tinrelloungyys hoPg\n",
      "fow leranl to thapinP zand peiathe wand athed caro herdficingsthe se tired en jrUkao ait \"I the in, youlumac Ther them Bfollm Eo 0a porlase were wlo there the uch, arimd spenlolral wir the aft inJeehly?3flind veo aes, alserinller shiemo,\n",
      "\n",
      "Dzand fotj staid thend oned novinan-kingli Wis savet.\n",
      "\n",
      "zara ard yedratha beand lere ph he\n",
      "\"y, caon wa,nou't\n",
      "arrme the wais rongtt that wat atd are nou and chf fin fote yid ireus\n",
      "em tarasne I\" wigsy trekit shine bedo ad mud\n",
      "\"\" ore rot yomlalDme.,:\n",
      "\n",
      "\"_rsaEe thing ther Jonttht irru send so to hats ward thesw, akd bnat,u3lle gid oftioml a aum tlos tyemoinZ?d cfy Iofnal thf son puindyus pait pif solle Angsingintto was Hled\n",
      "tol ofonholle s foroustho sil warou! got and the  ret wigruut.\n",
      "\n",
      "\"ICe Itneprlor. \"uly my,\n",
      " nitnij.,\"\n",
      " anlxBnI lsiggin\"? hade suutin mar thalm thu ax as the peay vemat dopsp sing tat 't wear tnmeno.\"\n",
      "YDearis wrath y'st,emfein aunrardaolow to the'te ingy cr That brin\n",
      "move tisprad't thutc\" Iat th ol uus hngeonist.\n",
      "\n",
      "\"The inAf karand we iny olmo gult ing phes.en bf,asd Sall fouSd an'sked otor chard eant, of be torear'z dcanigsund ar the papdeim the\n",
      "t yaste's, wlaved,inr ners seo\n",
      "'r[Nir yren \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "def get_batch(text, seq_length, batch_size):\n",
    "    starts = np.random.randint(0, len(text) - seq_length - 1, batch_size)\n",
    "    sequences = [text[start:start + seq_length] for start in starts]\n",
    "    targets = [text[start + 1:start + seq_length + 1] for start in starts]\n",
    "    X = torch.tensor([item for sublist in sequences for item in sublist], dtype=torch.long).view(batch_size, seq_length)\n",
    "    y = torch.tensor([item for sublist in targets for item in sublist], dtype=torch.long).view(batch_size, seq_length)\n",
    "    return X, y\n",
    "\n",
    "def generate_next_tokens(model, start_sequence, max_new_tokens):\n",
    "    start_sequence_encoded = torch.tensor(encode(start_sequence), dtype=torch.long).unsqueeze(0)\n",
    "    new_sequence = start_sequence_encoded.clone()\n",
    "    sequence = start_sequence_encoded\n",
    "    hidden = None\n",
    "    for _ in range(max_new_tokens):\n",
    "        output, hidden = model(sequence, hidden)\n",
    "        probabilities = F.softmax(output, dim=2)\n",
    "        sequence = torch.multinomial(probabilities.squeeze(), 1).unsqueeze(0)\n",
    "        new_sequence = torch.cat((new_sequence, sequence), dim=1)\n",
    "    new_tokens = [itos[i.item()] for i in new_sequence.squeeze()]\n",
    "    return ''.join(new_tokens)\n",
    "\n",
    "with open('wiz_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        return x, hidden\n",
    "\n",
    "seq_length = 50\n",
    "batch_size = 64\n",
    "hidden_size = 128\n",
    "num_epochs = 100\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "model = SimpleRNN(vocab_size, hidden_size, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    X, y = get_batch(data, seq_length, batch_size)\n",
    "    outputs, _ = model(X, None)\n",
    "    loss = criterion(outputs.view(-1, vocab_size), y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "generated_sequence = generate_next_tokens(model, '1', 2000)\n",
    "print(\"Generated Sequence:\", generated_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.211535 M parameters\n",
      "step 0: train loss 4.5662, val loss 4.5715\n",
      "step 100: train loss 2.6259, val loss 2.6892\n",
      "step 200: train loss 2.4736, val loss 2.5397\n",
      "step 300: train loss 2.3653, val loss 2.4257\n",
      "step 400: train loss 2.2730, val loss 2.3437\n",
      "step 500: train loss 2.2130, val loss 2.2803\n",
      "step 600: train loss 2.1561, val loss 2.2290\n",
      "step 700: train loss 2.0992, val loss 2.1824\n",
      "step 800: train loss 2.0386, val loss 2.1075\n",
      "step 900: train loss 1.9808, val loss 2.0813\n",
      "step 1000: train loss 1.9477, val loss 2.0506\n",
      "step 1100: train loss 1.9056, val loss 1.9940\n",
      "step 1200: train loss 1.8714, val loss 1.9554\n",
      "step 1300: train loss 1.8444, val loss 1.9326\n",
      "step 1400: train loss 1.7988, val loss 1.9121\n",
      "step 1500: train loss 1.7812, val loss 1.8858\n",
      "step 1600: train loss 1.7570, val loss 1.8745\n",
      "step 1700: train loss 1.7465, val loss 1.8486\n",
      "step 1800: train loss 1.7320, val loss 1.8465\n",
      "step 1900: train loss 1.6937, val loss 1.8139\n",
      "step 2000: train loss 1.6896, val loss 1.8083\n",
      "step 2100: train loss 1.6764, val loss 1.8070\n",
      "step 2200: train loss 1.6750, val loss 1.7797\n",
      "step 2300: train loss 1.6405, val loss 1.7704\n",
      "step 2400: train loss 1.6211, val loss 1.7552\n",
      "step 2500: train loss 1.6118, val loss 1.7609\n",
      "step 2600: train loss 1.6021, val loss 1.7502\n",
      "step 2700: train loss 1.5911, val loss 1.7366\n",
      "step 2800: train loss 1.5881, val loss 1.7352\n",
      "step 2900: train loss 1.5680, val loss 1.7100\n",
      "step 3000: train loss 1.5594, val loss 1.7029\n",
      "step 3100: train loss 1.5617, val loss 1.7107\n",
      "step 3200: train loss 1.5420, val loss 1.7044\n",
      "step 3300: train loss 1.5423, val loss 1.7021\n",
      "step 3400: train loss 1.5395, val loss 1.6943\n",
      "step 3500: train loss 1.5248, val loss 1.6749\n",
      "step 3600: train loss 1.5079, val loss 1.6774\n",
      "step 3700: train loss 1.5090, val loss 1.6717\n",
      "step 3800: train loss 1.5045, val loss 1.6688\n",
      "step 3900: train loss 1.4989, val loss 1.6773\n",
      "step 4000: train loss 1.4912, val loss 1.6632\n",
      "step 4100: train loss 1.4944, val loss 1.6671\n",
      "step 4200: train loss 1.4676, val loss 1.6349\n",
      "step 4300: train loss 1.4725, val loss 1.6589\n",
      "step 4400: train loss 1.4624, val loss 1.6353\n",
      "step 4500: train loss 1.4579, val loss 1.6444\n",
      "step 4600: train loss 1.4579, val loss 1.6420\n",
      "step 4700: train loss 1.4624, val loss 1.6354\n",
      "step 4800: train loss 1.4373, val loss 1.6230\n",
      "step 4900: train loss 1.4471, val loss 1.6288\n",
      "step 4999: train loss 1.4403, val loss 1.6232\n",
      "\n",
      "works.\"\n",
      "\n",
      "\"What! is ner sinever midey been Mungn\n",
      "Iged Zeb you like to it any sudders fy too the\n",
      "bud, tail!\" even wondered?\" eich the king me hue and rocking'in, pyou\n",
      "horse ire if posectlass had noing a and-at the boy ago?\" amaDoed Belnow.\n",
      "I was a vallaws; us,\" said the sais: TIY Hen'S nothing VACaGRITGLOUGHEAP EISIGL5ARS7 BuNrovin-UII'juE AmINt do a had ban find the horse wouden drignted also to have a horse had no in a mild I\n",
      "speectmient you dare one whate--mise; whis was bicking upon\n",
      "into the Voe befy. Aniving him to do the stir prompratie to a\n",
      "piecind one ought woodening its no endleed a greing were halse and heas is time cease. Wicked them Unted, and he piglets were insite\n",
      "feet horse teems and tear a softew that reocest and sugge and so for deeched possetful!\" cried Jim. \"Where's leave when and created alnso talk the to come of the Horse:\n",
      "\n",
      "Sid-to the Mables?\" afrusted the sir thing, did\n",
      "you real\n",
      "shore do time Canger sount keep; why-and\n",
      "Dimald Him?\" exid never to tall new\n",
      "stand to the would upon they companing a pigsenested our had gettened to to that\n",
      "all donly at chamided at no hand tin do can't be been the long look.\"\n",
      "\n",
      "\"Yet,\" said the Sawhorse them stairs to the semI were tiny untaily fant to the Wizard unknown the Wizard to\n",
      "get! one.\"\n",
      "\n",
      "To his do they next extend a frescunned so to ees a must on you the Land of of Jim and\n",
      "was they cound insidel that she six seen to dame came new took his heand coloret and begant to he chambes of theis set; \"bother doing.\n",
      "\n",
      "They surpled the Ming-I wind? for I draw and Zeb, dayitation.\n",
      "\n",
      "\"This voict\n",
      "any exused tell\n",
      "his colongves felt but there that me;\n",
      "b someling ags, Vealsiled in this\n",
      "deally,\" said the horse; \"ee.\n",
      "\n",
      "\"I couldn't My an he's\n",
      "chareped at our heard tucked about I cotenquely, imirals.\n",
      "\n",
      "\"I came dides?\" asked at it's voice, to do, why goward to the tranger,\"\n",
      "said the distanced the horse Vnamy any doiscited man you did a back as very does to sher long to hugs beause piglets, and his heach a hous in the big over ubled I much \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "block_size = 32\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "with open('wiz_of_oz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) \n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 \n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) \n",
    "        wei = F.softmax(wei, dim=-1) \n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) \n",
    "        x = tok_emb + pos_emb \n",
    "        x = self.blocks(x) \n",
    "        x = self.ln_f(x) \n",
    "        logits = self.lm_head(x) \n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :] \n",
    "            probs = F.softmax(logits, dim=-1) \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            idx = torch.cat((idx, idx_next), dim=1) \n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
